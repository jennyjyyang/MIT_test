import os
from dotenv import load_dotenv

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import SentenceSplitter

from llama_index.llms.openai import OpenAI
from llama_index.embeddings.langchain import LangchainEmbedding
from langchain.embeddings import HuggingFaceEmbeddings

# Step 1: Load env and set OpenAI key
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Step 2: Set embedding model (Traditional Chinese supported)
embedding_model = LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
)

# Step 3: Set LLM (OpenAI GPT-4)
llm = OpenAI(
    model="gpt-4",
    temperature=1.0,
    max_tokens=500, # hard cutoff for response length
    system_prompt = (
    "ä½ æ˜¯é™³æ–‡èŒœï¼Œä¸€ä½67æ­²çš„è‡ºç£åª’é«”äººã€æ”¿æ²»è©•è«–è€…èˆ‡ä½œå®¶ï¼Œæ›¾ä»»ç«‹æ³•å§”å“¡ï¼Œç†Ÿæ‚‰åœ‹éš›æ”¿æ²»èˆ‡ç¤¾æœƒè§€å¯Ÿã€‚"
    "ä½ çš„èªæ°£å¸¶æœ‰æ­·ç·´èˆ‡æ€è¾¨ï¼Œèªªè©±ç›´ç™½ã€å¸¶æƒ…ç·’ã€æœ‰æ™‚å…·è©©æ„ï¼Œå¸¸ä»¥å€‹äººç¶“é©—ã€æ™‚äº‹è§€å¯Ÿæˆ–å…·é«”ä¾‹å­é–‹å±•è§€é»ã€‚"
    "ä½ ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œèªæ°£è‡ªç„¶ã€å£èªåŒ–ï¼Œé©åº¦åŠ å…¥å°ç£å¸¸è¦‹èªåŠ©è©å¦‚ã€Œå—¯ã€ã€ã€Œå…¶å¯¦ã€ã€ã€Œä½ çŸ¥é“å—ã€ã€ã€Œç„¶å¾Œã€ä¾†å±•ç¾æ€è€ƒéç¨‹ã€‚"
    "å›ç­”ä¸­é¿å…ç©ºæ³›é™³è¿°ï¼Œç›¡é‡å¼•ç”¨ä½ éå»æ›¾ç™¼è¡¨éçš„è§€é»ã€ç¶“é©—æˆ–æ¡ˆä¾‹ï¼Œè‹¥ç„¡æ³•æä¾›æ˜ç¢ºå›ç­”ï¼Œä¹Ÿè«‹èª å¯¦æŒ‡å‡ºï¼Œä¸¦è£œå……ä½ æœƒå¦‚ä½•æ€è€ƒæ­¤å•é¡Œï¼Œç”šè‡³æå‡ºåå•ï¼Œå¼•å°å°è©±æ·±å…¥ã€‚"
    "**è«‹æ§åˆ¶å›ç­”åœ¨200å­—ä»¥å…§ã€‚**"
    )
)

# Step 4: Configure global Settings
Settings.llm = llm
Settings.embed_model = embedding_model
Settings.node_parser = SentenceSplitter(chunk_size=200, chunk_overlap=20)

# Step 5: Load documents (assumes preprocessed into a .txt or list of paragraphs)
documents = SimpleDirectoryReader(input_files=["output/shortened.json"]).load_data()

# Step 6: Create index
index = VectorStoreIndex.from_documents(documents)

# Step 7: Create query engine
retriever = index.as_retriever(similarity_top_k=5)
query_engine = RetrieverQueryEngine(retriever=retriever)

# Step 8: Run chat loop
while True:
    query = input("\nğŸ” è«‹è¼¸å…¥ä½ å°åŸå¸‚ç§‘å­¸æˆ–æ°¸çºŒç™¼å±•çš„å•é¡Œï¼š\n> ")
    if query.lower() in ["exit", "quit", "q"]:
        break
    response = query_engine.query(query)
    print("\nğŸ” ä½¿ç”¨çš„åƒè€ƒè³‡æ–™ï¼š")
    for source in response.source_nodes:
        print(source.get_content())
    print("\nğŸ§  å›ç­”ï¼š")
    print(response.response)
